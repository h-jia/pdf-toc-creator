1 绪论; 9
    1.1 例子:多项式曲线拟合; 10
    1.2 概率论; 16
        1.2.1 概率密度; 20
        1.2.2 期望和协方差; 21
        1.2.3 贝叶斯概率; 22
        1.2.4 高斯分布; 24
        1.2.5 重新考察曲线拟合问题; 26
        1.2.6 贝叶斯曲线拟合; 28
    1.3 模型选择; 29
    1.4 维度灾难; 30
    1.5 决策论; 33
        1.5.1 最小化错误分类率; 34
        1.5.2 最小化期望损失; 35
        1.5.3 拒绝选项; 35
        1.5.4 推断和决策; 36
        1.5.5 回归问题的损失函数; 38
    1.6 信息论; 39 
        1.6.1 相对熵和互信息; 44
    1.7 练习; 46
2 概率分布; 52
    2.1 二元变量; 52 
        2.1.1 Beta分布; 54
    2.2 多项式变量; 56 
        2.2.1 狄利克雷分布; 58
    2.3 高斯分布; 59
        2.3.1 条件高斯分布; 63
        2.3.2 边缘高斯分布; 65
        2.3.3 高斯变量的贝叶斯定理; 67
        2.3.4 高斯分布的最大似然估计; 69
        2.3.5 顺序估计; 69
        2.3.6 高斯分布的贝叶斯推断; 71
        2.3.7 学生t分布; 75
        2.3.8 周期变量; 77
        2.3.9 混合高斯模型; 81
    2.4 指数族分布; 83
        2.4.1 最大似然与充分统计量; 86
        2.4.2 共轭先验; 87
        2.4.3 无信息先验; 87
    2.5 非参数化方法; 89 
        2.5.1 核密度估计; 90 
        2.5.2 近邻方法; 92
    2.6 练习; 94
3 回归的线性模型; 101
    3.1 线性基函数模型; 101 
        3.1.1 最大似然与最小平方; 102 
        3.1.2 最小平方的几何描述; 105 
        3.1.3 顺序学习; 105
        3.1.4 正则化最小平方; 105
        3.1.5 多个输出; 106
    3.2 偏置-方差分解; 108
    3.3 贝叶斯线性回归; 111
        3.3.1 参数分布; 111 
        3.3.2 预测分布; 113 
        3.3.3 等价核; 116
    3.4 贝叶斯模型比较; 118
    3.5 证据近似; 121 
        3.5.1 计算证据函数; 121 
        3.5.2 最大化证据函数; 123 
        3.5.3 参数的有效数量; 124
    3.6 固定基函数的局限性; 126
    3.7 练习; 126
4 分类的线性模型; 130
    4.1 判别函数; 131 
        4.1.1 二分类; 131 
        4.1.2 多分类; 132 
        4.1.3 用于分类的最小平方方法; 133 
        4.1.4 Fisher线性判别函数; 135 
        4.1.5 与最小平方的关系; 137 
        4.1.6 多分类的Fisher判别函数; 138 
        4.1.7 感知器算法; 139
    4.2 概率生成式模型; 141 
        4.2.1 连续输入; 143 
        4.2.2 最大似然解; 144 
        4.2.3 离散特征; 146 
        4.2.4 指数族分布; 146
    4.3 概率判别式模型; 147 
        4.3.1 固定基函数; 147 
        4.3.2 logistic回归; 148 
        4.3.3 迭代重加权最小平方; 149 
        4.3.4 多类logistic回归; 150 
        4.3.5 probit回归; 151 
        4.3.6 标准链接函数; 152
    4.4 拉普拉斯近似; 154 
        4.4.1 模型比较和BIC; 155
    4.5 贝叶斯logistic回归; 156 
        4.5.1 拉普拉斯近似; 156 
        4.5.2 预测分布; 157
    4.6 练习; 158
5 神经网络; 161
    5.1 前馈神经网络; 161 
        5.1.1 权空间对称性; 165
    5.2 网络训练; 165 
        5.2.1 参数最优化; 168 
        5.2.2 局部二次近似; 169 
        5.2.3 使用梯度信息; 170 
        5.2.4 梯度下降最优化; 170
    5.3 误差反向传播; 171
        5.3.1 误差函数导数的计算; 172 
        5.3.2 一个简单的例子; 174 
        5.3.3 反向传播的效率; 175 
        5.3.4 Jacobian矩阵; 175
    5.4 Hessian矩阵; 177 
        5.4.1 对角近似; 177 
        5.4.2 外积近似; 178 
        5.4.3 Hessian矩阵的逆矩阵; 178 
        5.4.4 有限差; 179 
        5.4.5 Hessian矩阵的精确计算; 179 
        5.4.6 Hessian矩阵的快速乘法; 180
    5.5 神经网络的正则化; 182 
        5.5.1 相容的高斯先验; 183 
        5.5.2 早停止; 185 
        5.5.3 不变性; 186 
        5.5.4 切线传播; 187 
        5.5.5 用变换后的数据训练; 189 
        5.5.6 卷积神经网络; 190 
        5.5.7 软权值共享; 191
    5.6 混合密度网络; 193
    5.7 贝叶斯神经网络; 197 
        5.7.1 后验参数分布; 198 
        5.7.2 超参数最优化; 199 
        5.7.3 用于分类的贝叶斯神经网络; 200
    5.8 练习; 202
6 核方法; 206
    6.1 对偶表示; 206
    6.2 构造核; 207
    6.3 径向基函数网络; 211
        6.3.1 Nadaraya-Watson模型; 212
    6.4 高斯过程; 214 
        6.4.1 重新考虑线性回归问题; 214 
        6.4.2 用于回归的高斯过程; 216 
        6.4.3 学习超参数; 219 
        6.4.4 自动相关性确定; 220 
        6.4.5 用于分类的高斯过程; 221 
        6.4.6 拉普拉斯近似; 222 
        6.4.7 与神经网络的联系; 225
    6.5 练习; 225
7 稀疏核机; 228
    7.1 最大边缘分类器; 228 
        7.1.1 重叠类分布; 231 
        7.1.2 与logistic回归的关系; 235 
        7.1.3 多类SVM; 236 
        7.1.4 回归问题的SVM; 237 
        7.1.5 计算学习理论; 240
    7.2 相关向量机; 241 
        7.2.1 用于回归的RVM; 241 
        7.2.2 稀疏性分析; 244 
        7.2.3 RVM用于分类; 247
    7.3 练习; 249
8 图模型; 251
    8.1 贝叶斯网络; 251 
        8.1.1 例子:多项式回归; 253 
        8.1.2 生成式模型; 255 
        8.1.3 离散变量; 255 
        8.1.4 线性高斯模型; 257
    8.2 条件独立; 259 
        8.2.1 图的三个例子; 260 
        8.2.2 d-划分; 264
    8.3 马尔科夫随机场; 266 
        8.3.1 条件独立性质; 267 
        8.3.2 分解性质; 268 
        8.3.3 例子:图像去噪; 269 
        8.3.4 与有向图的关系; 271
    8.4 图模型中的推断; 274 
        8.4.1 链推断; 274 
        8.4.2 树; 277 
        8.4.3 因子图; 277 
        8.4.4 加和-乘积算法; 279 
        8.4.5 最大加和算法; 285 
        8.4.6 一般图的精确推断; 289 
        8.4.7 循环置信传播; 289 
        8.4.8 学习图结构; 290
    8.5 练习; 290
9 混合模型和EM; 293
    9.1 K均值聚类; 293 
        9.1.1 图像分割与压缩; 296
    9.2 混合高斯; 297 
        9.2.1 最大似然; 298 
        9.2.2 用于高斯混合模型的EM; 300
    9.3 EM的另一种观点; 303 
        9.3.1 重新考察高斯混合模型; 304 
        9.3.2 与K均值的关系; 305 
        9.3.3 伯努利分布的混合; 306 
        9.3.4 贝叶斯线性回归的EM算法; 309
    9.4 一般形式的EM算法; 310
    9.5 练习; 313
10 近似推断; 316 
    10.1变分推断; 316 
        10.1.1 分解概率分布; 317 
        10.1.2 分解近似的性质; 319 
        10.1.3 例子:一元高斯分布; 321 
        10.1.4 模型比较; 324
    10.2 例子:高斯的变分混合; 324 
        10.2.1 变分分布; 325 
        10.2.2 变分下界; 329 
        10.2.3 预测概率密度; 330 
        10.2.4 确定分量的数量; 331
        10.2.5 诱导分解; 332 
    10.3变分线性回归; 332 
        10.3.1 变分分布; 333 
        10.3.2 预测分布; 334 
        10.3.3 下界; 335 
    10.4 指数族分布; 335 
        10.4.1 变分信息传递; 337 
    10.5局部变分方法; 337 
    10.6 变分logistic回归; 341 
        10.6.1 变分后验概率分布; 341 
        10.6.2 最优化变分参数; 343 
        10.6.3 超参数的推断; 344 
    10.7期望传播; 346 
        10.7.1 例子:聚类问题; 350 
        10.7.2 图的期望传播; 352 
    10.8练习; 355
11 采样方法; 358 
    11.1基本采样算法; 359 
        11.1.1 标准概率分布; 359 
        11.1.2 拒绝采样; 361 
        11.1.3 可调节的拒绝采样; 362 
        11.1.4 重要采样; 363 
        11.1.5 采样-重要性-重采样; 365 
        11.1.6 采样与EM算法; 366
    11.2 马尔科夫链蒙特卡罗; 367 
        11.2.1 马尔科夫链; 368 
        11.2.2 Metropolis-Hastings算法; 370
    11.3 吉布斯采样; 370
    11.4切片采样; 373 
    11.5混合蒙特卡罗算法; 374 
        11.5.1 动态系统; 374 
        11.5.2 混合蒙特卡罗方法; 376 
    11.6估计划分函数; 378 
    11.7练习; 379
12 连续潜在变量; 381
    12.1 主成分分析; 381 
        12.1.1 最大方差形式; 382 
        12.1.2 最小误差形式; 383 
        12.1.3 PCA的应用; 385 
        12.1.4 高维数据的PCA; 388 
    12.2概率PCA; 388 
        12.2.1 最大似然PCA; 391 
        12.2.2 用于PCA的EM算法; 393 
        12.2.3 贝叶斯PCA; 395 
        12.2.4 因子分析; 397 
    12.3核PCA; 399 
    12.4 非线性隐含变量模型; 402 
        12.4.1 独立成分分析; 402 
        12.4.2 自关联网络; 403 
        12.4.3 对非线性流形建模; 405
    12.5练习; 407
13 顺序数据; 410 
    13.1马尔科夫模型; 410 
        13.2隐马尔科夫模型; 413
        13.2.1 用于HMM的最大似然法; 417 
        13.2.2 前向后向算法; 418 
        13.2.3 用于HMM的加和-乘积算法; 423 
        13.2.4 缩放因子; 425 
        13.2.5 维特比算法; 426 
        13.2.6 隐马尔科夫模型的扩展; 427
    13.3线性动态系统; 430 
        13.3.1 LDS中的推断; 432 
        13.3.2 LDS中的学习; 434 
        13.3.3 LDS的推广; 436 
        13.3.4 粒子滤波; 437
    13.4练习; 438
14 组合模型; 441 
    14.1贝叶斯模型平均; 441 
    14.2委员会; 442 
    14.3提升方法; 443
        14.3.1 最小化指数误差; 444
        14.3.2 提升方法的误差函数; 446 
    14.4基于树的模型; 447 
    14.5条件混合模型; 449
        14.5.1 线性回归模型的混合; 449 
    14.6 logistic模型的混合; 452 
        14.6.1 专家混合; 453 
    14.7练习; 454
A 附录A 数据集; 456 
    A.1 手写数字; 456 
    A.2 石油流; 456 
    A.3 老忠实间歇喷泉; 458 
    A.4 人工生成数据; 459
B 附录B 概率分布; 460 
    B.1 伯努利分布; 460 
    B.2 Beta分布; 460 
    B.3 二项分布; 461 
    B.4 狄利克雷分布; 461 
    B.5 Gamma分布; 462 
    B.6 高斯分布; 462 
    B.7 高斯-Gamma分布; 463 
    B.8 高斯-Wishart分布; 464 
    B.9 多项式分布; 464 
    B.10 正态分布; 464 
    B.11 学生t分布; 465 
    B.12 均匀分布; 465 
    B.13 VonMises分布; 465 
    B.14 Wishart分布; 466
C 附录C 矩阵的性质; 467 
    C.1 矩阵的基本性质; 467 
    C.2 迹和行列式; 467 
    C.3 矩阵的导数; 468 
    C.4 特征向量方程; 469
D 附录D 变分法; 472
E 附录E 拉格朗日乘数法; 474
